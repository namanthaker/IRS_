{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612eebf2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:10.755139Z",
     "iopub.status.busy": "2022-03-04T04:20:10.753264Z",
     "iopub.status.idle": "2022-03-04T04:20:10.768509Z",
     "shell.execute_reply": "2022-03-04T04:20:10.769081Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.026622Z"
    },
    "papermill": {
     "duration": 0.046808,
     "end_time": "2022-03-04T04:20:10.769364",
     "exception": false,
     "start_time": "2022-03-04T04:20:10.722556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ac9727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:10.825767Z",
     "iopub.status.busy": "2022-03-04T04:20:10.824764Z",
     "iopub.status.idle": "2022-03-04T04:20:21.220530Z",
     "shell.execute_reply": "2022-03-04T04:20:21.221042Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.045497Z"
    },
    "papermill": {
     "duration": 10.42506,
     "end_time": "2022-03-04T04:20:21.221230",
     "exception": false,
     "start_time": "2022-03-04T04:20:10.796170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re #for working with regular expression\n",
    "import nltk #for natural language processing (nlp)\n",
    "import spacy #also for nlp\n",
    "import string #This is a module, Python also has built-in class str, these are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbdb4f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:21.276766Z",
     "iopub.status.busy": "2022-03-04T04:20:21.276124Z",
     "iopub.status.idle": "2022-03-04T04:20:21.349669Z",
     "shell.execute_reply": "2022-03-04T04:20:21.348965Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.081117Z"
    },
    "papermill": {
     "duration": 0.102541,
     "end_time": "2022-03-04T04:20:21.349861",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.247320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#Read training data\n",
    "trdf=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv', header='infer')\n",
    "\n",
    "#Let us see what do we have. \n",
    "print(trdf.head(3))\n",
    "#Note that he have a text column, which we will use n this demo\n",
    "\n",
    "#Info on the training set\n",
    "trdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ceafa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:21.415969Z",
     "iopub.status.busy": "2022-03-04T04:20:21.415130Z",
     "iopub.status.idle": "2022-03-04T04:20:21.418425Z",
     "shell.execute_reply": "2022-03-04T04:20:21.418995Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.134652Z"
    },
    "papermill": {
     "duration": 0.043086,
     "end_time": "2022-03-04T04:20:21.419177",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.376091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this #earthquake m...\n",
      "1               forest fire near la ronge sask. canada\n",
      "2    all residents asked to 'shelter in place' are ...\n",
      "Name: lowered_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#how to covert everything in text in lower case\n",
    "trdf['lowered_text']=trdf['text'].str.lower()\n",
    "\n",
    "#confirm the case conversion\n",
    "print(trdf['lowered_text'].head(3))\n",
    "\n",
    "#you can notice case conversion in each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9258e72d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:21.519908Z",
     "iopub.status.busy": "2022-03-04T04:20:21.509202Z",
     "iopub.status.idle": "2022-03-04T04:20:21.525914Z",
     "shell.execute_reply": "2022-03-04T04:20:21.525354Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.180270Z"
    },
    "papermill": {
     "duration": 0.080155,
     "end_time": "2022-03-04T04:20:21.526057",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.445902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "<class 'dict'> {33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
      "0    our deeds are the reason of this #earthquake m...\n",
      "1               forest fire near la ronge sask. canada\n",
      "2    all residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    just got sent this photo from ruby #alaska as ...\n",
      "5    #rockyfire update => california hwy. 20 closed...\n",
      "6    #flood #disaster heavy rain causes flash flood...\n",
      "7    i'm on top of the hill and i can see a fire in...\n",
      "8    there's an emergency evacuation happening now ...\n",
      "9    i'm afraid that the tornado is coming to our a...\n",
      "Name: lowered_text, dtype: object\n",
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "5    rockyfire update  california hwy 20 closed in ...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7    im on top of the hill and i can see a fire in ...\n",
      "8    theres an emergency evacuation happening now i...\n",
      "9     im afraid that the tornado is coming to our area\n",
      "Name: lowered_text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#another way of doing the same thing\\ndef remove_punctuation(in_str):\\n    return in_str.translate(mapping)\\n\\nprint(trdf[\\'lowered_text\\'].head(10))   \\ntrdf[\\'lowered_text\\']=trdf[\"lowered_text\"].apply(lambda x: remove_punctuation(x))\\nprint(trdf[\\'lowered_text\\'].head(10))   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of punctuation\n",
    "\n",
    "#First get all punctuatipn\n",
    "punctuation=string.punctuation\n",
    "\n",
    "#Examine type and see all punctuation. Type is str.\n",
    "print(type(punctuation), punctuation)\n",
    "\n",
    "#Create mapping dictionary. As we have specified 3 arguments, each char in \n",
    "#the third argument is mapped to None\n",
    "#In case if we specify only 1 argument, that argument must be a dictinary\n",
    "#specifying 1 to 1 mapping for characters\n",
    "#In case of 2 arguments, both must be string specifying 1 to 1 char mapping\n",
    "\n",
    "mapping=str.maketrans(\"\",\"\",punctuation)\n",
    "\n",
    "#just check that a mapping is indeed a dictionary and print the dictionary, \n",
    "#it will show None as the value for each key and keys are different punctuation\n",
    "#, to be precise ascii value of punctuation\n",
    "print(type(mapping), mapping)\n",
    "\n",
    "#Print before and after translation, look at the translation code in between\n",
    "#print statement\n",
    "print(trdf['lowered_text'].head(10))    \n",
    "trdf['lowered_text']=trdf[\"lowered_text\"].str.translate(mapping)\n",
    "print(trdf['lowered_text'].head(10))    \n",
    "\n",
    "#Notice removal of punctuation in all lines\n",
    "\n",
    "\"\"\"\n",
    "#another way of doing the same thing\n",
    "def remove_punctuation(in_str):\n",
    "    return in_str.translate(mapping)\n",
    "\n",
    "print(trdf['lowered_text'].head(10))   \n",
    "trdf['lowered_text']=trdf[\"lowered_text\"].apply(lambda x: remove_punctuation(x))\n",
    "print(trdf['lowered_text'].head(10))   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3300b584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:21.589043Z",
     "iopub.status.busy": "2022-03-04T04:20:21.588361Z",
     "iopub.status.idle": "2022-03-04T04:20:21.630931Z",
     "shell.execute_reply": "2022-03-04T04:20:21.630374Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.237126Z"
    },
    "papermill": {
     "duration": 0.076177,
     "end_time": "2022-03-04T04:20:21.631072",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.554895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 7702\n",
      "<class 'list'> 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Let us have a look at standard list of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(type(stopwords.words()), len(stopwords.words()))\n",
    "#type is list of strings/words\n",
    "#print(stopwords.words()) #Very huge list (7702 stopwords) as it includes stopwords from many languages\n",
    "#and therefore not printing\n",
    "\n",
    "\n",
    "print(type(stopwords.words('english')), len(stopwords.words('english'))) #list, 179 stopwords\n",
    "\n",
    "#now fetch stopwords in English only\n",
    "print(stopwords.words('english'))\n",
    "stopwords_eng=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdc366b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:21.710863Z",
     "iopub.status.busy": "2022-03-04T04:20:21.705042Z",
     "iopub.status.idle": "2022-03-04T04:20:21.932658Z",
     "shell.execute_reply": "2022-03-04T04:20:21.933195Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.261654Z"
    },
    "papermill": {
     "duration": 0.274017,
     "end_time": "2022-03-04T04:20:21.933422",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.659405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "5    rockyfire update  california hwy 20 closed in ...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7    im on top of the hill and i can see a fire in ...\n",
      "8    theres an emergency evacuation happening now i...\n",
      "9     im afraid that the tornado is coming to our area\n",
      "Name: lowered_text, dtype: object\n",
      "0        deeds reason earthquake may allah forgive us \n",
      "1               forest fire near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "5    rockyfire update california hwy 20 closed dire...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7                          im top hill see fire woods \n",
      "8    theres emergency evacuation happening building...\n",
      "9                       im afraid tornado coming area \n",
      "Name: lowered_text_stop_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#you can remove stopwords using the following code and the new text is written in a new column\n",
    "#\"lowered_text_stop_removed\"\n",
    "\n",
    "#Before stop word removal\n",
    "print(trdf[\"lowered_text\"].head(10))\n",
    "\n",
    "#function to remove stop words\n",
    "def remove_stopwords(in_str):\n",
    "    new_str=''\n",
    "    words=in_str.split() #string is splitted through white space in a list of words\n",
    "    for tx in words:\n",
    "        if tx not in stopwords_eng:\n",
    "            new_str=new_str + tx + \" \"\n",
    "    return new_str\n",
    "\n",
    "#Call the stop word removal function on each row\n",
    "trdf['lowered_text_stop_removed']=trdf[\"lowered_text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#confirm the removal of stopwords\n",
    "print(trdf[\"lowered_text_stop_removed\"].head(10))\n",
    "#notice the removal of stopwords in line 0, 2, 4, 5, 7, 8 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a6b99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:22.042301Z",
     "iopub.status.busy": "2022-03-04T04:20:22.005826Z",
     "iopub.status.idle": "2022-03-04T04:20:22.055601Z",
     "shell.execute_reply": "2022-03-04T04:20:22.055048Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.495678Z"
    },
    "papermill": {
     "duration": 0.092901,
     "end_time": "2022-03-04T04:20:22.055752",
     "exception": false,
     "start_time": "2022-03-04T04:20:21.962851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "<class 'list'> [('like', 345), ('im', 299), ('amp', 298), ('fire', 250), ('get', 229), ('new', 224), ('via', 220), ('people', 196), ('one', 193), ('news', 193)]\n",
      "['like', 'im', 'amp', 'fire', 'get', 'new', 'via', 'people', 'one', 'news']\n"
     ]
    }
   ],
   "source": [
    "#Identify Frequent words. Sometimes we need to remove them as they may not be useful because\n",
    "#they are general in nature and appear in every row (or document in general sense). This can mean that\n",
    "#they are not useful in conveying specific theme of a row/document. \n",
    "#We can identify them and may remove them\n",
    "\n",
    "#import Counter from collections\n",
    "from collections import Counter\n",
    "\n",
    "#Create instance of Counter\n",
    "counter=Counter()\n",
    "\n",
    "#Count frequency of every word\n",
    "for text in trdf[\"lowered_text_stop_removed\"]:\n",
    "    for word in text.split():\n",
    "        counter[word]+=1\n",
    "\n",
    "#Check type of counter, it is collections.Counter\n",
    "print(type(counter))\n",
    "\n",
    "#get the list with 10 most frequent word. List is a list of (10) tuples, where the first value in each \n",
    "#tuple is a word and the second value is its frequency. 10 tuples because argument below is 10\n",
    "most_cmn_list=counter.most_common(10)\n",
    "\n",
    "#Check the type and the list\n",
    "print(type(most_cmn_list), most_cmn_list) #type is list (list of tuples/word,frequency pair)\n",
    "\n",
    "#Separate words from frequency and put words in a separate list most_cmn_words_list\n",
    "most_cmn_words_list=[]\n",
    "\n",
    "for word, freq in most_cmn_list:\n",
    "    most_cmn_words_list.append(word)\n",
    "\n",
    "#Check to see that words are separated\n",
    "print(most_cmn_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beaf60cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:22.161493Z",
     "iopub.status.busy": "2022-03-04T04:20:22.158365Z",
     "iopub.status.idle": "2022-03-04T04:20:22.165705Z",
     "shell.execute_reply": "2022-03-04T04:20:22.166390Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.563185Z"
    },
    "papermill": {
     "duration": 0.081922,
     "end_time": "2022-03-04T04:20:22.166622",
     "exception": false,
     "start_time": "2022-03-04T04:20:22.084700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        deeds reason earthquake may allah forgive us \n",
      "1               forest fire near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "5    rockyfire update california hwy 20 closed dire...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7                          im top hill see fire woods \n",
      "8    theres emergency evacuation happening building...\n",
      "9                       im afraid tornado coming area \n",
      "Name: lowered_text_stop_removed, dtype: object\n",
      "0        deeds reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 receive wildfires evacuation orders cali...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "5    rockyfire update california hwy 20 closed dire...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7                                  top hill see woods \n",
      "8    theres emergency evacuation happening building...\n",
      "9                          afraid tornado coming area \n",
      "Name: lowered_text_stop_removed_freq_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove top 10 frequent words (You may decide the number for your problem/data. This is a demo with 10)\n",
    "\n",
    "#Before removing 10 most common words\n",
    "print(trdf['lowered_text_stop_removed'].head(10))\n",
    "\n",
    "#function to remove words\n",
    "def remove_frequent(in_str):\n",
    "    new_str=''\n",
    "    for word in in_str.split():\n",
    "        if word not in most_cmn_words_list:\n",
    "            new_str=new_str + word + \" \"\n",
    "    return new_str\n",
    "\n",
    "#Apply the function for removal of 10 most coomon words and store the resultant series in\n",
    "#trdf[\"lowered_text_stop_removed_freq_removed\"]\n",
    "\n",
    "trdf[\"lowered_text_stop_removed_freq_removed\"]=trdf['lowered_text_stop_removed'].apply(lambda x: remove_frequent(x))\n",
    "\n",
    "\n",
    "#Confirm the removal of most common 10 words\n",
    "print(trdf[\"lowered_text_stop_removed_freq_removed\"].head(10))\n",
    "#Notice removal of people from row with id 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2c1da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:22.242827Z",
     "iopub.status.busy": "2022-03-04T04:20:22.242051Z",
     "iopub.status.idle": "2022-03-04T04:20:22.245360Z",
     "shell.execute_reply": "2022-03-04T04:20:22.246010Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.616848Z"
    },
    "papermill": {
     "duration": 0.04781,
     "end_time": "2022-03-04T04:20:22.246233",
     "exception": false,
     "start_time": "2022-03-04T04:20:22.198423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['httptcofa3fcnln86', '9km', 'httptco5uecmcv2pk', 'forney', 'developing', 'symptoms', 'httptcorqkk15uhey', 'httptconf4iculoje', 'httptcostfmbbzfb5', 'httptcoymy4rskq3d']\n"
     ]
    }
   ],
   "source": [
    "#identify most 10 rare words\n",
    "#Sometimes they are actually not words in a dictionary, and therefore better to identify and remove.\n",
    "\n",
    "#-10 to end, means most rare/less frequent 10\n",
    "most_rare_list=counter.most_common()[-10:] #list of tuples\n",
    "\n",
    "#separate words from tuple\n",
    "most_rare_words=[]\n",
    "for word, freq in most_rare_list:\n",
    "    most_rare_words.append(word)\n",
    "\n",
    "print(most_rare_words)\n",
    "#You can notice they are really not words and therefore will mostly be not helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad91369a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:22.355488Z",
     "iopub.status.busy": "2022-03-04T04:20:22.354490Z",
     "iopub.status.idle": "2022-03-04T04:20:22.358276Z",
     "shell.execute_reply": "2022-03-04T04:20:22.358767Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.632788Z"
    },
    "papermill": {
     "duration": 0.081764,
     "end_time": "2022-03-04T04:20:22.358984",
     "exception": false,
     "start_time": "2022-03-04T04:20:22.277220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        deeds reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 receive wildfires evacuation orders cali...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "5    rockyfire update california hwy 20 closed dire...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7                                  top hill see woods \n",
      "8    theres emergency evacuation happening building...\n",
      "9                          afraid tornado coming area \n",
      "Name: lowered_stop_freq_rare_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#remove rare words\n",
    "\n",
    "#define the function to remove rare words from each row\n",
    "def remove_rare(in_text):\n",
    "    new_text=\"\"\n",
    "    for word in in_text.split():\n",
    "        if word not in most_rare_words:\n",
    "            new_text=new_text + word + \" \"\n",
    "    return new_text\n",
    "\n",
    "#call the function to remove rare words from each of the row in trdf[\"lowered_text_stop_removed_freq_removed\"]\n",
    "#and store the result in trdf[\"lowered_stop_freq_rare_removed\"]\n",
    "trdf[\"lowered_stop_freq_rare_removed\"]=trdf[\"lowered_text_stop_removed_freq_removed\"].apply(lambda x: remove_rare(x))\n",
    "print(trdf[\"lowered_stop_freq_rare_removed\"].head(10))\n",
    "#There was no rare word in first 10 lines, so no change here but in some rows furhter down, there are\n",
    "#changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a713a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:22.444401Z",
     "iopub.status.busy": "2022-03-04T04:20:22.433376Z",
     "iopub.status.idle": "2022-03-04T04:20:24.649440Z",
     "shell.execute_reply": "2022-03-04T04:20:24.648883Z",
     "shell.execute_reply.started": "2022-03-04T04:11:55.687939Z"
    },
    "papermill": {
     "duration": 2.260206,
     "end_time": "2022-03-04T04:20:24.649585",
     "exception": false,
     "start_time": "2022-03-04T04:20:22.389379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        deeds reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 receive wildfires evacuation orders cali...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "Name: lowered_stop_freq_rare_removed, dtype: object\n",
      "0           deed reason earthquak may allah forgiv us \n",
      "1                     forest near la rong sask canada \n",
      "2    resid ask shelter place notifi offic evacu she...\n",
      "3         13000 receiv wildfir evacu order california \n",
      "4    got sent photo rubi alaska smoke wildfir pour ...\n",
      "Name: Stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Now Stemming using PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Create instance of a PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "#Before Stemming\n",
    "print(trdf[\"lowered_stop_freq_rare_removed\"].head(5))\n",
    "\n",
    "def do_stemming(in_str):\n",
    "    new_str=\"\"\n",
    "    for word in in_str.split():\n",
    "        new_str=new_str + stemmer.stem(word) + \" \"\n",
    "    return new_str\n",
    "\n",
    "trdf[\"Stemmed\"]=trdf[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_stemming(x))\n",
    "\n",
    "#Confirm after stemming\n",
    "print(trdf[\"Stemmed\"].head(5))\n",
    "#Note changes in the output, you may not be happy, another option is SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7803b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:24.714760Z",
     "iopub.status.busy": "2022-03-04T04:20:24.714098Z",
     "iopub.status.idle": "2022-03-04T04:20:26.040003Z",
     "shell.execute_reply": "2022-03-04T04:20:26.039431Z",
     "shell.execute_reply.started": "2022-03-04T04:11:57.924922Z"
    },
    "papermill": {
     "duration": 1.359503,
     "end_time": "2022-03-04T04:20:26.040168",
     "exception": false,
     "start_time": "2022-03-04T04:20:24.680665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "0        deeds reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 receive wildfires evacuation orders cali...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "Name: lowered_stop_freq_rare_removed, dtype: object\n",
      "0           deed reason earthquak may allah forgiv us \n",
      "1                     forest near la rong sask canada \n",
      "2    resid ask shelter place notifi offic evacu she...\n",
      "3         13000 receiv wildfir evacu order california \n",
      "4    got sent photo rubi alaska smoke wildfir pour ...\n",
      "Name: Stemmed_sb, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#If you did not like Porter stemmer, we also have a SnowballStemmer\n",
    "#it supports many languages unlike Porter stemmer which supports only English\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#print languages supported\n",
    "print(SnowballStemmer.languages)\n",
    "\n",
    "#create instance of a SnowBallStemmer\n",
    "stemmer_sb=SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "#Before Stemming\n",
    "print(trdf[\"lowered_stop_freq_rare_removed\"].head(5))\n",
    "\n",
    "def do_stemming_sb(in_str):\n",
    "    new_str=\"\"\n",
    "    for word in in_str.split():\n",
    "        new_str=new_str + stemmer_sb.stem(word) + \" \"\n",
    "    return new_str\n",
    "\n",
    "trdf[\"Stemmed_sb\"]=trdf[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_stemming_sb(x))\n",
    "\n",
    "#Confirm after stemming\n",
    "print(trdf[\"Stemmed_sb\"].head(5))\n",
    "#Note changes in the output, you still may not be happy, another option, Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f7661c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:26.113234Z",
     "iopub.status.busy": "2022-03-04T04:20:26.112534Z",
     "iopub.status.idle": "2022-03-04T04:20:28.816194Z",
     "shell.execute_reply": "2022-03-04T04:20:28.815391Z",
     "shell.execute_reply.started": "2022-03-04T04:11:59.259518Z"
    },
    "papermill": {
     "duration": 2.744476,
     "end_time": "2022-03-04T04:20:28.816400",
     "exception": false,
     "start_time": "2022-03-04T04:20:26.071924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        deeds reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 receive wildfires evacuation orders cali...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "Name: lowered_stop_freq_rare_removed, dtype: object\n",
      "0          deed reason earthquake may allah forgive u \n",
      "1                    forest near la ronge sask canada \n",
      "2    resident asked shelter place notified officer ...\n",
      "3    13000 receive wildfire evacuation order califo...\n",
      "4    got sent photo ruby alaska smoke wildfire pour...\n",
      "Name: Lemmatized, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#didn't like any of the stemmers, you may use Lemmatization. \n",
    "#Lemmatization is similar to stemming and reduces inflected words to their word stem\n",
    "#but differs in the way that it makes sure the root word (also called as lemma) belongs\n",
    "#to the language. However, because of this, it is slow.\n",
    "\n",
    "#import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#create instance of a WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "#Before Lemmatizing\n",
    "print(trdf[\"lowered_stop_freq_rare_removed\"].head(5))\n",
    "\n",
    "def do_lemmatizing(in_str):\n",
    "    new_str=\"\"\n",
    "    for word in in_str.split():\n",
    "        new_str=new_str + lem.lemmatize(word) + \" \"\n",
    "    return new_str\n",
    "\n",
    "trdf[\"Lemmatized\"]=trdf[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_lemmatizing(x))\n",
    "\n",
    "#Confirm after stemming\n",
    "print(trdf[\"Lemmatized\"].head(5))\n",
    "#Note changes in the output. Still not happy, see the next cell\n",
    "\n",
    "trdf[\"Lemmatized\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176ffef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:20:28.895367Z",
     "iopub.status.busy": "2022-03-04T04:20:28.894680Z",
     "iopub.status.idle": "2022-03-04T04:21:14.920998Z",
     "shell.execute_reply": "2022-03-04T04:21:14.920211Z",
     "shell.execute_reply.started": "2022-03-04T04:12:02.010451Z"
    },
    "papermill": {
     "duration": 46.070136,
     "end_time": "2022-03-04T04:21:14.921189",
     "exception": false,
     "start_time": "2022-03-04T04:20:28.851053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('deeds', 'NNS'), ('reason', 'NN'), ('earthquake', 'NN'), ('may', 'MD'), ('allah', 'VB'), ('forgive', 'JJ'), ('us', 'PRP')]\n",
      "('deeds', 'NNS')\n",
      "N\n",
      "0         deed reason earthquake may allah forgive us \n",
      "1                    forest near la ronge sask canada \n",
      "2    resident asked shelter place notified officers...\n",
      "3    13000 receive wildfire evacuation orders calif...\n",
      "4    got sent photo ruby alaska smoke wildfire pour...\n",
      "Name: Lemmatized, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can do Lemmatization considering POS tags, that will be a bit better\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Creating a dictionary below. We wish to work with 4 main categories of words only\n",
    "#We don't want to bother about other categories as well as sub-categories of the \n",
    "#four main categories (e.g. in part-of-speech tagging there are sub-categories of noun like NN, NNS etc.)\n",
    "#, we are planning to work.\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "#Just to understand, let us get the POS tags for the first row/line\n",
    "pos_tagged_text = nltk.pos_tag(trdf.loc[0,\"lowered_stop_freq_rare_removed\"].split())\n",
    "\n",
    "#print the type and POS tags\n",
    "print(type(pos_tagged_text)) #It is a list (list of tuples)\n",
    "print(pos_tagged_text) #List of tuples with the first element of the tuple being the word and the second\n",
    "#element being its POS tag\n",
    "\n",
    "#Let us undestand slicing of POS tags list, as we will require to pick the first leeter of the POS tag\n",
    "#because we have created the dictionary above (wordnet_map) that way\n",
    "print(pos_tagged_text[0]) #0th tuple\n",
    "print(pos_tagged_text[0][1][0]) #Zeorth letter of POS tag of the first word \n",
    "#First index is which tuple, second index is which value in the tuple selected using first index\n",
    "#third index is which character in the POS tag selected using first and second index\n",
    "\n",
    "def do_lemmatizing_with_POS(in_str):\n",
    "    new_str=''\n",
    "    for word in in_str.split():\n",
    "        tag=nltk.pos_tag(word)[0][1][0]\n",
    "        new_str=new_str + lem.lemmatize(word, wordnet_map.get(tag,wordnet.NOUN)) + \" \"\n",
    "    return new_str\n",
    "\n",
    "trdf[\"Lemmatized\"]=trdf[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_lemmatizing_with_POS(x))\n",
    "\n",
    "#Confirm after stemming\n",
    "print(trdf[\"Lemmatized\"].head(5))\n",
    "#Note changes in the output. Still not happy, see the next cell\n",
    "\n",
    "trdf[\"Lemmatized\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba326ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:21:14.998392Z",
     "iopub.status.busy": "2022-03-04T04:21:14.997628Z",
     "iopub.status.idle": "2022-03-04T04:21:15.003708Z",
     "shell.execute_reply": "2022-03-04T04:21:15.004220Z",
     "shell.execute_reply.started": "2022-03-04T04:12:54.058586Z"
    },
    "papermill": {
     "duration": 0.048507,
     "end_time": "2022-03-04T04:21:15.004408",
     "exception": false,
     "start_time": "2022-03-04T04:21:14.955901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on \\\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of emojis\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "#and\n",
    "#https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing/notebook\n",
    "\n",
    "#For more details on how to write regular expression, you can refer the following:\n",
    "#https://docs.microsoft.com/en-us/visualstudio/ide/using-regular-expressions-in-visual-studio?view=vs-2022\n",
    "#The link is for REs in .NET but useful in general sense also.\n",
    "#Following can also be referred:\n",
    "#https://www.geeksforgeeks.org/regular-expression-python-examples-set-1/\n",
    "\n",
    "def remove_emoji(in_str):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE) #re.compile creates a pattern object\n",
    "    return emoji_pattern.sub(r'\\\\n', in_str) #substitutes with ' '. \n",
    "#Here r in (r’’) stands for raw string literal (which ignores (stops parsing) escape sequence, e.g.\n",
    "#print('\\n') # prints a newline character but print(r'\\n') # prints \\n because parsing of escape sequence\n",
    "#is ignored).\n",
    "#here if we remove r, emoji is replaced by \\n because escape sequence will be parsed\n",
    "#but if we keep r, emoji will be replaced by \\\\n\n",
    "#The first argument is the string to be used for substitution and the second argument is the string\n",
    "#in which substitution is to be done (not in place, therefore return is necessary)\n",
    "#please not that the RE above is analogous to [abc]+ which matches to a, aa, ab ... etc.\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85268fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:21:15.076528Z",
     "iopub.status.busy": "2022-03-04T04:21:15.075908Z",
     "iopub.status.idle": "2022-03-04T04:21:15.081395Z",
     "shell.execute_reply": "2022-03-04T04:21:15.081898Z",
     "shell.execute_reply.started": "2022-03-04T04:12:54.076718Z"
    },
    "papermill": {
     "duration": 0.042759,
     "end_time": "2022-03-04T04:21:15.082079",
     "exception": false,
     "start_time": "2022-03-04T04:21:15.039320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ordinal and One-Hot Encodings for Categorical Data: '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+') #Note pipe in between means 2 options\n",
    "    #first one is https?://one or more repeatations of non-spaces\n",
    "    #second one is www.one or more repeatations of non-spaces\n",
    "    #\\. will ensure that it is treated as . not any its special meaning - any character\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "text = \"Ordinal and One-Hot Encodings for Categorical Data: https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7313e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:21:15.155375Z",
     "iopub.status.busy": "2022-03-04T04:21:15.154694Z",
     "iopub.status.idle": "2022-03-04T04:21:15.161576Z",
     "shell.execute_reply": "2022-03-04T04:21:15.162320Z",
     "shell.execute_reply.started": "2022-03-04T04:12:54.100941Z"
    },
    "papermill": {
     "duration": 0.045662,
     "end_time": "2022-03-04T04:21:15.162533",
     "exception": false,
     "start_time": "2022-03-04T04:21:15.116871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":target:before { content:&quot;&quot;; display:block; height:150px; margin:-150px 0 0; } h3 {font-weight:normal; margin-top:.5em} h4 { font-weight:lighter }\n",
      "\n",
      "\n",
      "\n",
      "This tutorial covers the operations you have perform on categorical data before it can be used in an ML algorithm. But there is more to it. You will also have to clean your data. If you would like to know more about this process, be sure to take a look at DataCamp's Cleaning Data in Python course.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing HTML tags\n",
    "def remove_html(in_str):\n",
    "    html_pattern = re.compile('<.*?>') #pattern is <zero or more ocuurance of any\n",
    "    #charater in lazy matching style>\n",
    "    return html_pattern.sub(r'', in_str)\n",
    "\n",
    "text = \"\"\"<div id=\"scoped-content\">\n",
    "<style type=\"text/css\">:target:before { content:&quot;&quot;; display:block; height:150px; margin:-150px 0 0; } h3 {font-weight:normal; margin-top:.5em} h4 { font-weight:lighter }\n",
    "</style>\n",
    "<br />\n",
    "\n",
    "<p>This tutorial covers the operations you have perform on categorical data before it can be used in an ML algorithm. But there is more to it. You will also have to clean your data. If you would like to know more about this process, be sure to take a look at DataCamp's <a href=\"https://www.datacamp.com/courses/cleaning-data-in-python\">Cleaning Data in Python</a> course.</p>\n",
    "<p><a id=\"categorical\"></a></p>\"\"\"\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e64971a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:21:15.235992Z",
     "iopub.status.busy": "2022-03-04T04:21:15.235356Z",
     "iopub.status.idle": "2022-03-04T04:23:45.065421Z",
     "shell.execute_reply": "2022-03-04T04:23:45.064611Z"
    },
    "papermill": {
     "duration": 149.867735,
     "end_time": "2022-03-04T04:23:45.065571",
     "exception": false,
     "start_time": "2022-03-04T04:21:15.197836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f8394adc150>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspellchecker/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f8394adc4d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspellchecker/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f8394adc810>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspellchecker/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f8394adcb50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspellchecker/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f8394adce90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspellchecker/\u001b[0m\r\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pyspellchecker (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for pyspellchecker\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2918d300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:23:45.148696Z",
     "iopub.status.busy": "2022-03-04T04:23:45.147513Z",
     "iopub.status.idle": "2022-03-04T04:23:45.530776Z",
     "shell.execute_reply": "2022-03-04T04:23:45.530078Z",
     "shell.execute_reply.started": "2022-03-04T04:12:54.119573Z"
    },
    "papermill": {
     "duration": 0.428382,
     "end_time": "2022-03-04T04:23:45.531095",
     "exception": true,
     "start_time": "2022-03-04T04:23:45.102713",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20/2312864240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#create instance of SpellChecker class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "#Spelling correction\n",
    "\n",
    "#import\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "#create instance of SpellChecker class\n",
    "spell = SpellChecker()\n",
    "\n",
    "#function to correct spelling\n",
    "def correct_spellings(in_str):\n",
    "    new_str = \"\"\n",
    "    #first create list of unknown words in the list\n",
    "    misspelled_words = spell.unknown(in_str.split())\n",
    "    for word in in_str.split():\n",
    "        if word in misspelled_words:\n",
    "            new_str = new_str + spell.correction(word) + \" \"\n",
    "        else:\n",
    "            new_str = new_str + word + \" \"\n",
    "    return new_str\n",
    "        \n",
    "text = \"speling correctin\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651a406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:15:28.921234Z",
     "iopub.status.busy": "2022-03-04T04:15:28.920714Z",
     "iopub.status.idle": "2022-03-04T04:15:28.971645Z",
     "shell.execute_reply": "2022-03-04T04:15:28.970915Z",
     "shell.execute_reply.started": "2022-03-04T04:15:28.921184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import pyplot from matplotlib and WordCloud from wordcloud\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#ensure in-line plotting\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db70c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:15:33.560342Z",
     "iopub.status.busy": "2022-03-04T04:15:33.560049Z",
     "iopub.status.idle": "2022-03-04T04:15:41.997270Z",
     "shell.execute_reply": "2022-03-04T04:15:41.996340Z",
     "shell.execute_reply.started": "2022-03-04T04:15:33.560307Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create instance of the WordCloud() \n",
    "word_cloud=WordCloud(height=1080, width=2048, background_color='white')\n",
    "\n",
    "#get the text in a big string\n",
    "\n",
    "text=\" \".join([str(word) for word in trdf[\"Lemmatized\"]])\n",
    "\n",
    "print(type(text))\n",
    "\n",
    "#generate the word cloud\n",
    "word_cloud.generate(text)\n",
    "\n",
    "#store the word cloud in a file\n",
    "word_cloud.to_file('cloud1.png')\n",
    "\n",
    "#display now\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Common Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90f0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T04:15:52.301975Z",
     "iopub.status.busy": "2022-03-04T04:15:52.301478Z",
     "iopub.status.idle": "2022-03-04T04:18:22.401273Z",
     "shell.execute_reply": "2022-03-04T04:18:22.399729Z",
     "shell.execute_reply.started": "2022-03-04T04:15:52.301933Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "wget.download('https://storage.googleapis.com/kagglesdsdata/datasets/24780/31574/loc.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20220203%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220203T092136Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=845e1f0b75ed48a5911da682e16426682c57a15c36dde0e134cd40fdea9f2f3f3beabe7b3e5357c44de92b5bae55325d1455194da244b6716e245556207e582990463ba35dc48e52f707b7b73e7c930636910cb6021d3ced756a280132cc68f372bea07430ab421758176092dbe4c7c5636fb2e203629218a4f4ed945c079d0832439bc3beb4d1db836e5bc13996bcd7b9da46f2106463fa85aba76e0f7246f42d8ecd39c9a03abab8f678f7a7ab499798fc3104219914f92b46750954ecf10606490a530b37d4eac976aa40d03cc2bfca7b505f050fac02c1c4860f353f8becdb9c64d0d99f191d3331cea5287bdc7883063bb75e514ff6f9c57d6d2f0d6b67')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c883e88",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#word cloud in a desired shape\n",
    "\n",
    "#read a background image first\n",
    "\n",
    "import cv2\n",
    "\n",
    "mask_array=np.array(cv2.imread('loc.png'))\n",
    "\n",
    "word_cloud=WordCloud(height=1080, width=2048, background_color='white', mask=mask_array)\n",
    "\n",
    "\n",
    "#generate the word cloud\n",
    "word_cloud.generate(text)\n",
    "\n",
    "#image_colors=ImageColorGenerator(mask_array)\n",
    "#word_cloud.recolor(color_func=image_colors)\n",
    "\n",
    "#store the word cloud in a file\n",
    "word_cloud.to_file('cloud2.png')\n",
    "\n",
    "#display now\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Common Words\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 227.638451,
   "end_time": "2022-03-04T04:23:48.935149",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-04T04:20:01.296698",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
